{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script for the WaveNet network on the VCTK corpus.\n",
    "\n",
    "This script trains a network with the WaveNet using data from the VCTK corpus,\n",
    "which can be freely downloaded at the following site (~10 GB):\n",
    "http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "import IPython.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "from wavenet import WaveNetModel, AudioReader, optimizer_factory, mu_law_encode, mu_law_decode, audio_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 16000\n",
    "TEMPERATURE = 1.0\n",
    "LOGDIR = './logdir'\n",
    "WAVENET_PARAMS = './wavenet_params.json'\n",
    "SAVE_EVERY = 8000\n",
    "SILENCE_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_dist(sp1, sp2):\n",
    "    m = min(sp1.shape[0], sp2.shape[0])\n",
    "    print (sp1.shape, sp2.shape, m)\n",
    "    return np.linalg.norm(sp1[:m, :]-sp2[:m, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments(args):\n",
    "    def _str_to_bool(s):\n",
    "        \"\"\"Convert string to bool (in argparse context).\"\"\"\n",
    "        if s.lower() not in ['true', 'false']:\n",
    "            raise ValueError('Argument needs to be a '\n",
    "                             'boolean, got {}'.format(s))\n",
    "        return {'true': True, 'false': False}[s.lower()]\n",
    "\n",
    "    def _ensure_positive_float(f):\n",
    "        \"\"\"Ensure argument is a positive float.\"\"\"\n",
    "        if float(f) < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                    'Argument must be greater than zero')\n",
    "        return float(f)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='WaveNet generation script')\n",
    "    parser.add_argument(\n",
    "        'checkpoint', type=str, help='Which model checkpoint to generate from')\n",
    "    parser.add_argument(\n",
    "        '--samples',\n",
    "        type=int,\n",
    "        default=SAMPLES,\n",
    "        help='How many waveform samples to generate')\n",
    "    parser.add_argument(\n",
    "        '--temperature',\n",
    "        type=_ensure_positive_float,\n",
    "        default=TEMPERATURE,\n",
    "        help='Sampling temperature')\n",
    "    parser.add_argument(\n",
    "        '--logdir',\n",
    "        type=str,\n",
    "        default=LOGDIR,\n",
    "        help='Directory in which to store the logging '\n",
    "        'information for TensorBoard.')\n",
    "    parser.add_argument(\n",
    "        '--wavenet_params',\n",
    "        type=str,\n",
    "        default=WAVENET_PARAMS,\n",
    "        help='JSON file with the network parameters')\n",
    "    parser.add_argument(\n",
    "        '--wav_out_path',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Path to output wav file')\n",
    "    parser.add_argument(\n",
    "        '--save_every',\n",
    "        type=int,\n",
    "        default=SAVE_EVERY,\n",
    "        help='How many samples before saving in-progress wav')\n",
    "    parser.add_argument(\n",
    "        '--fast_generation',\n",
    "        type=_str_to_bool,\n",
    "        default=True,\n",
    "        help='Use fast generation')\n",
    "    parser.add_argument(\n",
    "        '--wav_seed',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='The wav file to start generation from')\n",
    "    parser.add_argument(\n",
    "        '--gc_channels',\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help='Number of global condition embedding channels. Omit if no '\n",
    "             'global conditioning.')\n",
    "    parser.add_argument(\n",
    "        '--gc_cardinality',\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help='Number of categories upon which we globally condition.')\n",
    "    parser.add_argument(\n",
    "        '--gc_id',\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help='ID of category to generate, if globally conditioned.')\n",
    "    parser.add_argument(\n",
    "        '--lc_channels', \n",
    "        type=int, \n",
    "        default=0,\n",
    "        help='Number of local condition channels. Should be consistent with the local condition file provided. Default: 0')\n",
    "    parser.add_argument(\n",
    "        '--lc_path', \n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='The path to the local condition csv file (no header). If not provided, assume no local condition.') \n",
    "    parser.add_argument(\n",
    "        '--compare_path', \n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='The path to the wave file for comparison with generation.') \n",
    "    parser.add_argument(\n",
    "        '--lower_bound', \n",
    "        type=int,\n",
    "        default=None,\n",
    "        help='The lowerbound for log probability of sample. Any probability below e^lower_bound will be cleared to 0.' \n",
    "             'If not provided, assume no lower bound.') \n",
    "    arguments = parser.parse_args(args)\n",
    "    if arguments.gc_channels is not None:\n",
    "        if arguments.gc_cardinality is None:\n",
    "            raise ValueError(\"Globally conditioning but gc_cardinality not \"\n",
    "                             \"specified. Use --gc_cardinality=377 for full \"\n",
    "                             \"VCTK corpus.\")\n",
    "\n",
    "        if arguments.gc_id is None:\n",
    "            raise ValueError(\"Globally conditioning, but global condition was \"\n",
    "                              \"not specified. Use --gc_id to specify global \"\n",
    "                              \"condition.\")\n",
    "\n",
    "    return arguments\n",
    "\n",
    "\n",
    "def write_wav(waveform, sample_rate, filename):\n",
    "    y = np.array(waveform)\n",
    "    librosa.output.write_wav(filename, y, sample_rate)\n",
    "    print('Updated wav file at {}'.format(filename))\n",
    "\n",
    "\n",
    "def create_seed(filename,\n",
    "                sample_rate,\n",
    "                quantization_channels,\n",
    "                window_size,\n",
    "                silence_threshold=SILENCE_THRESHOLD):\n",
    "    audio, _ = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    audio = audio_reader.trim_silence(audio, silence_threshold)\n",
    "\n",
    "    quantized = mu_law_encode(audio, quantization_channels)\n",
    "    cut_index = tf.cond(tf.size(quantized) < tf.constant(window_size),\n",
    "                        lambda: tf.size(quantized),\n",
    "                        lambda: tf.constant(window_size))\n",
    "\n",
    "    return quantized[:cut_index]\n",
    "\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    args = get_arguments(argv)\n",
    "    \n",
    "    if args.lc_channels > 0:\n",
    "        if args.lc_path is not None:\n",
    "            raw_lc = pd.read_csv(args.lc_path, sep=',',header=None).values\n",
    "        else:\n",
    "            print (\n",
    "                ValueError('Location condition is enabled,' \n",
    "                           'and a local condition file must be provided.'))\n",
    "        lc = audio_reader.align_local_condition(raw_lc, args.samples)\n",
    "\n",
    "        lc_placeholder = tf.placeholder(tf.float32)\n",
    "    else:\n",
    "        lc = None\n",
    "        lc_placeholder = None\n",
    "\n",
    "    started_datestring = \"{0:%Y-%m-%dT%H-%M-%S}\".format(datetime.now())\n",
    "    logdir = os.path.join(args.logdir, 'generate', started_datestring)\n",
    "    with open(args.wavenet_params, 'r') as config_file:\n",
    "        wavenet_params = json.load(config_file)\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    net = WaveNetModel(\n",
    "        batch_size=1,\n",
    "        dilations=wavenet_params['dilations'],\n",
    "        filter_width=wavenet_params['filter_width'],\n",
    "        residual_channels=wavenet_params['residual_channels'],\n",
    "        dilation_channels=wavenet_params['dilation_channels'],\n",
    "        quantization_channels=wavenet_params['quantization_channels'],\n",
    "        skip_channels=wavenet_params['skip_channels'],\n",
    "        use_biases=wavenet_params['use_biases'],\n",
    "        scalar_input=wavenet_params['scalar_input'],\n",
    "        initial_filter_width=wavenet_params['initial_filter_width'],\n",
    "        global_condition_channels=args.gc_channels,\n",
    "        global_condition_cardinality=args.gc_cardinality,\n",
    "        local_condition_channels=args.lc_channels)\n",
    "\n",
    "    samples = tf.placeholder(tf.int32)\n",
    "\n",
    "    if args.fast_generation:\n",
    "        next_sample = net.predict_proba_incremental(samples, args.gc_id, local_condition = lc_placeholder)\n",
    "    else:\n",
    "        next_sample = net.predict_proba(samples, args.gc_id,  local_condition = lc_placeholder)\n",
    "\n",
    "    if args.fast_generation:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(net.init_ops)\n",
    "\n",
    "    variables_to_restore = {\n",
    "        var.name[:-2]: var for var in tf.global_variables()\n",
    "        if not ('state_buffer' in var.name or 'pointer' in var.name)}\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    print('Restoring model from {}'.format(args.checkpoint))\n",
    "    saver.restore(sess, args.checkpoint)\n",
    "\n",
    "    decode = mu_law_decode(samples, wavenet_params['quantization_channels'])\n",
    "\n",
    "    quantization_channels = wavenet_params['quantization_channels']\n",
    "    if args.wav_seed:\n",
    "        seed = create_seed(args.wav_seed,\n",
    "                           wavenet_params['sample_rate'],\n",
    "                           quantization_channels,\n",
    "                           net.receptive_field)\n",
    "        waveform = sess.run(seed).tolist()\n",
    "    else:\n",
    "        # Silence with a single random sample at the end.\n",
    "        waveform = [quantization_channels / 2] * (net.receptive_field - 1)\n",
    "        waveform.append(np.random.randint(quantization_channels))\n",
    "    \n",
    "    if lc is not None:\n",
    "        waveform_lc = [[0] * args.lc_channels] * (net.receptive_field - 1)\n",
    "    \n",
    "    probs = []\n",
    "\n",
    "    if args.fast_generation and args.wav_seed:\n",
    "        # When using the incremental generation, we need to\n",
    "        # feed in all priming samples one by one before starting the\n",
    "        # actual generation.\n",
    "        # TODO This could be done much more efficiently by passing the waveform\n",
    "        # to the incremental generator as an optional argument, which would be\n",
    "        # used to fill the queues initially.\n",
    "        outputs = [next_sample]\n",
    "        outputs.extend(net.push_ops)\n",
    "\n",
    "        print('Priming generation...')\n",
    "        for i, x in enumerate(waveform[-net.receptive_field: -1]):\n",
    "            if i % 100 == 0:\n",
    "                print('Priming sample {}'.format(i))\n",
    "            sess.run(outputs, feed_dict={samples: x})\n",
    "        print('Done.')\n",
    "\n",
    "    true_wav = None\n",
    "    true_mfcc = None\n",
    "    encoded_true_wav = None\n",
    "    if args.compare_path:\n",
    "        true_wav, _ = librosa.load(args.compare_path, sr=wavenet_params['sample_rate'])\n",
    "        true_mfcc = librosa.feature.mfcc(true_wav, sr=wavenet_params['sample_rate'], n_mfcc=args.lc_channels).T\n",
    "        print(true_wav.shape, true_mfcc.shape)\n",
    "        encoded_true_wav = mu_law_encode(true_wav, wavenet_params['quantization_channels'])\n",
    "        encoded_true_wav = net._one_hot(encoded_true_wav)\n",
    "        # Log ground truth\n",
    "        tf.summary.image(\n",
    "            \"Ground_Truth\",\n",
    "            tf.reshape(\n",
    "                encoded_true_wav, \n",
    "                [1, -1, wavenet_params['quantization_channels'], 1]),\n",
    "            max_outputs=1\n",
    "        )\n",
    "        IPython.display.display(Audio(true_wav, rate=wavenet_params['sample_rate']))\n",
    "\n",
    "    last_sample_timestamp = datetime.now()\n",
    "    for step in range(args.samples):\n",
    "        if args.lc_channels > 0:\n",
    "            waveform_lc.append(list(lc[step, :]))\n",
    "        if args.fast_generation:\n",
    "            outputs = [next_sample]\n",
    "            outputs.extend(net.push_ops)\n",
    "            window = waveform[-1]\n",
    "            if args.lc_channels > 0:\n",
    "                lc_window = np.reshape(lc[step], (1, -1))\n",
    "        else:\n",
    "            if len(waveform) > net.receptive_field:\n",
    "                window = waveform[-net.receptive_field:]\n",
    "                if args.lc_channels > 0:\n",
    "                    lc_window = np.reshape(waveform_lc[-net.receptive_field:], \n",
    "                                           (1, -1, args.lc_channels))\n",
    "            else:\n",
    "                window = waveform\n",
    "                if args.lc_channels > 0:\n",
    "                    lc_window = np.reshape(waveform_lc, (1, -1, args.lc_channels))\n",
    "            outputs = [next_sample]\n",
    "        \n",
    "        # Run the WaveNet to predict the next sample.\n",
    "        if args.lc_channels > 0:\n",
    "            prediction = sess.run(outputs, feed_dict={samples: window, lc_placeholder: lc_window})[0]\n",
    "        else:\n",
    "            prediction = sess.run(outputs, feed_dict={samples: window})[0]\n",
    "\n",
    "        # Scale prediction distribution using temperature.\n",
    "        np.seterr(divide='ignore')\n",
    "        scaled_prediction = np.log(prediction) / args.temperature\n",
    "#         if args.lower_bound:\n",
    "#             scaled_prediction[scaled_prediction < args.lower_bound] = -np.inf\n",
    "        scaled_prediction = (scaled_prediction -\n",
    "                             np.logaddexp.reduce(scaled_prediction))\n",
    "        scaled_prediction = np.exp(scaled_prediction)\n",
    "        np.seterr(divide='warn')\n",
    "\n",
    "        # Prediction distribution at temperature=1.0 should be unchanged after\n",
    "        # scaling.\n",
    "        if args.temperature == 1.0:\n",
    "            np.testing.assert_allclose(\n",
    "                    prediction, scaled_prediction, atol=1e-5,\n",
    "                    err_msg='Prediction scaling at temperature=1.0 '\n",
    "                            'is not working as intended.')\n",
    "\n",
    "        sample = np.random.choice(\n",
    "            np.arange(quantization_channels), p=scaled_prediction)\n",
    "        waveform.append(sample)\n",
    "        probs.append(scaled_prediction)\n",
    "        \n",
    "        # Show progress only once per second.\n",
    "        current_sample_timestamp = datetime.now()\n",
    "        time_since_print = current_sample_timestamp - last_sample_timestamp\n",
    "        if time_since_print.total_seconds() > 1.:\n",
    "            print('Sample {:3<d}/{:3<d}'.format(step + 1, args.samples), end='\\r')\n",
    "            last_sample_timestamp = current_sample_timestamp\n",
    "\n",
    "        # If we have partial writing, save the result so far.\n",
    "        if (args.wav_out_path and args.save_every and\n",
    "                (step + 1) % args.save_every == 0):\n",
    "            out = sess.run(decode, feed_dict={samples: waveform})\n",
    "            write_wav(out, wavenet_params['sample_rate'], args.wav_out_path)\n",
    "            \n",
    "            IPython.display.display(Audio(out, rate=wavenet_params['sample_rate']))\n",
    "\n",
    "##            Loss doesn't make sense here\n",
    "#             loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "#                                         logits=tf.convert_to_tensor(np.array(probs), np.float32),\n",
    "#                                         labels=tf.convert_to_tensor(encoded_true_wav[:, :step+1, :], np.float32))\n",
    "#             reduced_loss = tf.reduce_mean(loss)\n",
    "#             print(sess.run(reduced_loss))\n",
    "            out_mfcc = librosa.feature.mfcc(out[net.receptive_field:], sr=wavenet_params['sample_rate'], n_mfcc=args.lc_channels).T\n",
    "            print(mfcc_dist(out_mfcc, raw_lc))\n",
    "            if args.compare_path:\n",
    "                print(mfcc_dist(out_mfcc, true_mfcc), mfcc_dist(raw_lc, true_mfcc))\n",
    "            \n",
    "    # Introduce a newline to clear the carriage return from the progress.\n",
    "    print()\n",
    "\n",
    "    # Save the result as an audio summary.\n",
    "    datestring = str(datetime.now()).replace(' ', 'T')\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])\n",
    "    \n",
    "    # Log prediction\n",
    "    tf.summary.image(\n",
    "        \"Predicted_Probabilities\",\n",
    "        tf.reshape(\n",
    "            np.array(probs), \n",
    "            [1, -1, wavenet_params['quantization_channels'], 1]),\n",
    "        max_outputs=1\n",
    "    )\n",
    "\n",
    "    summaries = tf.summary.merge_all()\n",
    "    summary_out = sess.run(summaries,\n",
    "                           feed_dict={samples: np.reshape(waveform, [-1, 1])})\n",
    "    writer.add_summary(summary_out)\n",
    "\n",
    "    out = sess.run(decode, feed_dict={samples: waveform})\n",
    "\n",
    "    # Save the result as a wav file.\n",
    "    if args.wav_out_path:\n",
    "        write_wav(out, wavenet_params['sample_rate'], args.wav_out_path)\n",
    "\n",
    "    print('Finished generating. The result can be viewed in TensorBoard.')\n",
    "    IPython.display.display(Audio(out, rate=wavenet_params['sample_rate']))\n",
    "    \n",
    "## Loss doesn't make sense here\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "#                                         logits=tf.convert_to_tensor(np.array(probs), np.float32),\n",
    "#                                         labels=tf.convert_to_tensor(encoded_true_wav, np.float32))\n",
    "#     reduced_loss = tf.reduce_mean(loss)\n",
    "#     print(sess.run(reduced_loss))\n",
    "    out_mfcc = librosa.feature.mfcc(out[net.receptive_field:], sr=wavenet_params['sample_rate'], n_mfcc=args.lc_channels).T\n",
    "    print(mfcc_dist(out_mfcc, raw_lc))\n",
    "    if args.compare_path:\n",
    "        print(mfcc_dist(out_mfcc, true_mfcc), mfcc_dist(raw_lc, true_mfcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args for p225, mfcc local conditions and small architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../VCTK-Corpus/wav48/p225/p225_001.wav\"])\n",
    "# argv.extend(['--lc_channels', \"20\"])\n",
    "# argv.extend(['--lc_path', \"../VCTK-Corpus/mfcc/p225/p225_001.csv\"])\n",
    "# argv.extend(['--samples', \"32825\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_p225_mfcc_fix_146950_p225_001.wav\"])\n",
    "# argv.extend([\"logdir/train/p225_mfcc_fix/model.ckpt-146950\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../VCTK-Corpus/wav48/p225/p225_366.wav\"])\n",
    "# argv.extend(['--lc_channels', \"20\"])\n",
    "# argv.extend(['--lc_path', \"../VCTK-Corpus/mfcc/p225/p225_366.csv\"])\n",
    "# argv.extend(['--samples', \"84799\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_p225_mfcc_fix_149200_p225_366.wav\"])\n",
    "# argv.extend([\"logdir/train/p225_mfcc_fix/model.ckpt-149200\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../cmu_us_slt_arctic/wav/arctic_a0164.wav\"])\n",
    "# argv.extend(['--lc_channels', \"40\"])\n",
    "# argv.extend(['--lc_path', \"../cmu_us_slt_arctic/mfcc40/arctic_a0164.csv\"])\n",
    "# argv.extend(['--samples', \"53681\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_p225_cmu_mfcc40_99999_arctic_a0164.wav\"])\n",
    "# argv.extend([\"logdir/train/cmu_mfcc40/model.ckpt-99999\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../cmu_us_slt_arctic/wav/arctic_a0164.wav\"])\n",
    "# argv.extend(['--lc_channels', \"40\"])\n",
    "# argv.extend(['--lc_path', \"../cmu_us_slt_arctic/mfcc40_48k/arctic_a0164.csv\"])\n",
    "# argv.extend(['--samples', \"53681\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_cmu_mfcc40_48k_93200_arctic_a0164.wav\"])\n",
    "# argv.extend([\"logdir/train/cmu_mfcc40_48k/model.ckpt-93200\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args for p225, mgc local conditions and small architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../VCTK-Corpus/wav48/p225/p225_366.wav\"])\n",
    "# argv.extend(['--lc_channels', \"26\"])\n",
    "# argv.extend(['--lc_path', \"../VCTK-Corpus/mgc/p225/p225_366.csv\"])\n",
    "# argv.extend(['--samples', \"84799\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_p225_mgc_fix_3900_p225_366.wav\"])\n",
    "# argv.extend([\"logdir/train/p225_mgc_fix/model.ckpt-3900\"])\n",
    "# print(argv)\n",
    "# main(argv)\n",
    "\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../VCTK-Corpus/wav48/p225/p225_001.wav\"])\n",
    "# argv.extend(['--lc_channels', \"26\"])\n",
    "# argv.extend(['--lc_path', \"../VCTK-Corpus/mgc/p225/p225_001.csv\"])\n",
    "# argv.extend(['--samples', \"32825\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_p225_mgc_fix_3850_p225_001.wav\"])\n",
    "# argv.extend([\"logdir/train/p225_mgc_fix/model.ckpt-3850\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args for mfcc local conditions (25+1 coeff, 16kï¼Œ hop_length = 80, frame_length = 1024) and small architecture, CMU Dataset\n",
    "* Output prob gets lower bounded and power 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# argv =[]\n",
    "# argv.extend(['--temperature', \"1\"])\n",
    "# argv.extend(['--compare_path', \"../cmu_us_slt_arctic/wav/arctic_a0164.wav\"])\n",
    "# argv.extend(['--lc_channels', \"26\"])\n",
    "# argv.extend(['--lc_path', \"../cmu_us_slt_arctic/mfcc25+1_16k/arctic_a0164.csv\"])\n",
    "# argv.extend(['--samples', \"53681\"])\n",
    "# argv.extend(['--fast_generation', \"False\"])\n",
    "# argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "# argv.extend(['--wav_out_path', \"./results/gen_CMU_alt_mfcc25+1_16k_99999_arctic_a0164.wav\"])\n",
    "# argv.extend([\"logdir/train/CMU_alt_mfcc25+1_16k/model.ckpt-99999\"])\n",
    "# print(argv)\n",
    "# main(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "argv =[]\n",
    "argv.extend(['--temperature', \"1\"])\n",
    "argv.extend(['--compare_path', \"../cmu_us_slt_arctic/wav/arctic_a0164.wav\"])\n",
    "argv.extend(['--lc_channels', \"26\"])\n",
    "argv.extend(['--lc_path', \"../cmu_us_slt_arctic/mfcc25+1_16k/arctic_a0164.csv\"])\n",
    "argv.extend(['--samples', \"53681\"])\n",
    "argv.extend(['--fast_generation', \"False\"])\n",
    "argv.extend(['--wavenet_params', \"./wavenet_params2.json\"])\n",
    "argv.extend(['--wav_out_path', \"./results/gen_CMU_alt_no_noise_retrain_mfcc25+1_16k_84356_arctic_a0164.wav\"])\n",
    "argv.extend([\"logdir/train/CMU_alt_no_noise_retrain_mfcc25+1_16k/model.ckpt-84356\"])\n",
    "print(argv)\n",
    "main(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wavenet-ENV",
   "language": "python",
   "name": "tf-wavenet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
